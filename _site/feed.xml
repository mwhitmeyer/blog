<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://mwhitmeyer.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mwhitmeyer.github.io/blog/" rel="alternate" type="text/html" /><updated>2021-08-23T17:57:46-07:00</updated><id>https://mwhitmeyer.github.io/blog/feed.xml</id><title type="html">etcsetera</title><subtitle>theoretical computer science, but mainly etcetera</subtitle><author><name>Michael Whitmeyer</name></author><entry><title type="html">Efficiently Detecting Obfuscated Plagiarism</title><link href="https://mwhitmeyer.github.io/blog/plagiarism/2021/08/23/obfuscated-plagiarism.html" rel="alternate" type="text/html" title="Efficiently Detecting Obfuscated Plagiarism" /><published>2021-08-23T08:45:13-07:00</published><updated>2021-08-23T08:45:13-07:00</updated><id>https://mwhitmeyer.github.io/blog/plagiarism/2021/08/23/obfuscated-plagiarism</id><content type="html" xml:base="https://mwhitmeyer.github.io/blog/plagiarism/2021/08/23/obfuscated-plagiarism.html">&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
figure {
  border: 1px #000000 solid;
  padding: 4px;
  margin: auto;
}

figcaption {
  background-color: rgb(248, 247, 174);
  color: rgb(0, 0, 0);
  font-style: italic;
  padding: 2px;
  text-align: center;
}
&lt;/style&gt;
&lt;/head&gt;
&lt;/html&gt;



&lt;p&gt;

  Hi! It's been awhile. Quarantine hasn't been all fun and daisies for me personally (or most other people I imagine), but I am trying to get myself motivated and back into the swing of things. But I am not here to bore you with all that. Suffice it to say that hopefully I will start posting more regularly now as I head into my PhD! 

&lt;/p&gt;

&lt;p&gt;

  Now, this post will be about what I learned and created this summer during my internship at &lt;a href=&quot;https://www.stottlerhenke.com/&quot;&gt;Stottler Henke&lt;/a&gt; (SHAI). In general, I don't plan on discussing such (gasp) &lt;em&gt;real-world applicable&lt;/em&gt; things as much going forward on this blog, in case anyone was concerned.

&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Problem Setup&lt;/h2&gt;


&lt;p&gt;
  Now, let's talk about plagiarism. Here's the setup: we have a (very) large corpus (say, Wikipedia) of $N$ &quot;sensitive&quot; or &quot;source&quot; documents, which may or may not have been plagiarized by our query document. When we say plagiarism, we don't necessarily mean the whole document -- it could just be that a key sentence was copied, or a paragraph with crucial information was summarized at some (potentially random) point within the query document. Now, for exact matches, this is a relatively straightforward task, although it takes some clever data structures that I won't discuss in this post to get the runtime to depend only (linearly) on the length $m$ of the query document rather than $N$ (all the source documents we have to search through). This is what had already been accomplished when I arrived for my internship, and my task was to see what might be possible when our job was not just ot find exact matches, but also &quot;fuzzy&quot; matches. After all, a smart plagiarizer would try and hide their illicit behavior. But what does &quot;fuzzy&quot; even mean? So glad you asked, and if you have a good definition please send it my way.
&lt;/p&gt;

&lt;p&gt;
  Nevertheless, onwards. It seemed as good a starting place as any was the &lt;a href=&quot;https://pan.webis.de/clef14/pan14-web/&quot;&gt;PAN Conference&lt;/a&gt;, a conference whose central theme in 2011-&lt;a href=&quot;http://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-PotthastEt2013.pdf&quot;&gt;&lt;/a&gt;2015&lt;/a&gt; was plagiarism detection! Now, this seemed to be more of a competition and less of a thoroughly peer-reviewed venue, but nonetheless there were some interesting ideas floating around there, in case you want to see other things outside of what I will talk about in this post. In addition, they had a nice (albeit small) dataset and evaluation framework (which were a bit difficult to find but not impossible) that allows for an objective scoring and evaluation of any algorithm one might come up with! Beyond the dataset (and floating ideas), and perhaps most importantly, they also had a very nice framework for plagiarism detection, which I will now describe.
&lt;/p&gt;

&lt;p&gt;
  The framework they describe splits plagiarism detection into two main tasks: 

  &lt;ol&gt;
    &lt;li&gt;&lt;b&gt;Source Retrieval:&lt;/b&gt; This refers to the task of (efficiently) narrowing our search down from $N$ (which is just &lt;em&gt;overwhelmingly&lt;/em&gt; large) possible source documents to just $k$ (which we think of as some reasonable constant). We will do a more detailed plagiarism detection analysis on these $k$ documents in the next phase. You can think about this phase a sort of google search with the query document as the input and the source documents as the possible &quot;websites&quot;. Of course, like anyone ever, we only have time to look at the first page (a.k.a. the first $k$) results that Google spits out. &lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Text Alignment:&lt;/b&gt; This refers to the task of, well, actually &quot;finding&quot; the instances of plagiarism between the query document and the top $k$ source documents. This is where the fun really begins!&lt;/li&gt;
  &lt;/ol&gt;


  &lt;figure&gt;
    &lt;img src=&quot;../../../../img/posts/plagiarism/src-retrieve-text-align.png&quot; alt=&quot;There's supposed to be a visualization of source retrieval and text alignment here.&quot; style=&quot;width:100%&quot;&gt;
    &lt;figcaption&gt;A visualization of the source retrieval and text alignment process described above, from the PAN conference summary papers.&lt;/figcaption&gt;
  &lt;/figure&gt;
 
&lt;br&gt;

  In this post we will discuss some general strategies for source retrieval, but all of them involve some sort of &quot;search engine&quot; such as ElasticSearch or Lucene or maybe Google has an API (I dunno just pick your favorite one), and go a bit more into depth on some ideas I had for the text alignment phase.
&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Source Retrieval&lt;/h2&gt;
&lt;p&gt;
I think some discussion of the source retrieval step is warranted here, but I will try and keep this section brief and focus more on the algorithms I think are interesting for the text alignment phase. 

&lt;/p&gt;

&lt;p&gt;
  Basically, we just need to figure out how to input/give our query document to whatever search engine we are using. The search engine I was tasked with using had a finite limit on how many words the query could be, which is part of what makes things slightly tricky about this. Let's talk about some possible solutions:

  &lt;ul&gt;
    &lt;li&gt;
      If our document is very long, we can narrow it down to the top $w$ keywords (for your choice of $w$) in a fairly straightforward manner, depending on the search engine you are using or how you are indexing the source documents. Basically, the idea is to get the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;TF-IDF&lt;/a&gt; scores of each word in they query document, and make a new query that's just based on the top $w$ words according to this score. What is TF-IDF? So glad you asked. TF stands for &lt;b&gt;term frequency&lt;/b&gt;, which is just the number of times the term appears in the query document. IDF, which stands for &lt;b&gt;inverse document frequency&lt;/b&gt;, requires data from the indexed source documents, and it is equal to 
      $$\log \left ( \frac{n+1}{m+1}+1\right) $$
      where $n =$ number of source documents and $m=$ how many (source) documents the term appears in. If your index (again, Lucene, etc.) gives you quick access to $m$ and $n$ then you are good to go! Note that in this setup, &lt;em&gt;rare&lt;/em&gt; words (w.r.t. the overall corpus) are rewarded much more highly than words (such as &quot;the&quot; or &quot;he&quot;) that appear in almost every document. Also note that this method doesn't take into account the ordering of the words in the query document.
    &lt;/li&gt;
    &lt;li&gt;
      The previous method is fairly fast, and only requires one use of the search engine after you have created your new query. However, imagine that someone has hidden a small plagiarized passage about a violin within a very long article (which hasn't been plagiarized) about football. It is overwhelmingly likely that the violin words are &quot;drowned out&quot; by all the football, and the above method won't work. Now this is a particularly difficult kind of plagiarism to detect, but what good would our tool be if we just gave up on the tough examples? Plus, people usually try to hide their plagiarism. &lt;br&gt; 
      The next idea is an attempt to address this. It is simple: just split the query document into small enough chunks (of desired length), and treat each as a separate query to your search engine/index. This is much more likely to catch the nefarious example I described above, but has the drawback of being much slower and scaling with the length of the query document. One must also decide how to decide which results to prioritize between the chunks. You could take the ones with the highest overall scores (according to your search engine), but that may ignore some chunks altogether. You could also take the top few results from each chunk, but if your query is very long this could get very expensive later in the text alignment phase (in which we have to do detailed analysis on every document retrieved in this stage).
    &lt;/li&gt;

    &lt;li&gt;

      There are probably lot's of other ideas to optimize this step, and this step is actually HUGELY important when dealing with a massive corpus. Going back to my violin-inside-football-article example, even if you manage to catch the few sentences about a violin, there are probably thousands upon thousands of Wikipedia articles that deal with violins or even have them as the main topic. Source retrieval can be &lt;b&gt;&lt;em&gt;tough&lt;/em&gt;&lt;/b&gt;. One thing I didn't have time to try but which may be effective is extracting &lt;em&gt;named entities&lt;/em&gt;: for example, the TF-IDF method would try &quot;George&quot;, &quot;Washington&quot;, and &quot;Carver&quot; as unrelated words, but obviously if they appear one after another then we're talking about the badass, ahead-of-his-time &lt;a href=&quot;https://en.wikipedia.org/wiki/George_Washington_Carver&quot;&gt;legend himself&lt;/a&gt;.
    &lt;/li&gt;
  &lt;/ul&gt;

  
&lt;/p&gt;


&lt;h2 class=&quot;section-heading&quot;&gt;Text Alignment&lt;/h2&gt;

Ok, now we can talk about more fun stuff! Or at least, stuff we have a bit more control over. A small note: I think it makes a lot of sense to do all matching on a word-by-word basis (rather than character-by-character) if we are looking to detect obfuscated plagiarism. Why I think this will become even more clear when we discuss Smith-Waterman score functions, but this also makes sense because we can then take out punctuations, whitespaces, and &quot;stopwords&quot; (pronouns, prepositions, etc.), and reduce our document to something much simpler.

&lt;h3 class=&quot;subsection-heading&quot;&gt;Greedy Tiling&lt;/h3&gt;

&lt;p&gt;
  I am basically going to briefly summarize what I think are the important takeaways from &lt;a href=&quot;https://www.researchgate.net/publication/262763983_String_Similarity_via_Greedy_String_Tiling_and_Running_Karp-Rabin_Matching&quot;&gt;this paper&lt;/a&gt; by Wise. It's going to be &lt;em&gt;brief&lt;/em&gt;, and I mean it this time. The setup is that we have some source document, and some query document, and we want to find exact matches between them efficiently.
&lt;/p&gt;



&lt;p&gt;
  The idea is &lt;em&gt;super&lt;/em&gt; simple: just take the longest exact match first, &quot;tile&quot; it (meaning mark it so that no matches touch that area in the source or query document again), and repeat until there are no more matches of at least the minimum match length $t$. That's it. Some notes:
  &lt;ul&gt;
    &lt;li&gt;
      Technically, the worst case runtime of the most basic form of the algorithm is $O(n^3)$, if $n$ is the length of both the query and source documents. Which is bad. However, with some tricks, we can improve the worst case runtime to $O(n^2)$ (still bad), but crucially, the average case runtime (empirically) is $O(n)$ (good!). The rough idea is as follows: create an inverted index of both the query and source document mapping each phrase of length $t$ to the indices it occurs at. I should note that this can be accomplished via a &lt;a href=&quot;https://en.wikipedia.org/wiki/Rolling_hash&quot;&gt;rolling hash function&lt;/a&gt; for a very speedy mapping process, but since I was using Python dictionaries I didn't bother with this. Then you just run through the query phrases, checking if any appear in the source dictionary, and if so expanding them if necessary. I am hiding some details, but this is the gist of it. TLDR: it's pretty much linear time.
    &lt;/li&gt;
    &lt;li&gt;
      This algorithm is not technically optimal. Here, by &quot;optimal&quot;, we mean that it doesn't necessarily &lt;em&gt;maximally tile&lt;/em&gt; the query document. Here is a concrete example: consider $\mathbf{src} = b a a d c a a a a b a a$ and $\mathbf{query} = c a a b a a d $. Greedy tiling will find the match $a a b a a$ rather than the two matches $caa$ and $baad$. So, not technically optimal, but one could argue that we might want to find longer matches over shorter matches anyway. Also, this is a pretty pathological example.
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/p&gt;

&lt;h3 class=&quot;subsection-heading&quot;&gt;Smith-Waterman&lt;/h3&gt;

&lt;p&gt;
  This is probably my favorite algorithm of the bunch. Here's a &lt;a href=&quot;https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm&quot;&gt;wikipedia article&lt;/a&gt; on it, if you're into that sort of thing, but I will do my best that you don't need to reference it. The &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0022283681900875&quot;&gt;original paper&lt;/a&gt; is also all of 1.5 pages long, a real banger if you ask me.
&lt;/p&gt;



&lt;p&gt;
  My one line description of this algorithm would be &quot;The classic edit distance dynamic programming algorithm but with user-defined scoring&quot;. Since we're talking dynamic programming, we fill in an $m+1 \times n+1$ matrix $H$ of scores, where $m$ is the length of the query, and $n$ is the length of the source. We reward matching words, and penalize mismatching words and whenever we have to introduce a &quot;gap&quot;, meaning skip over a word in the query or source document.
  $$H_{i,j} = \max \begin{cases}
      H_{i-1,j-1} + \mathbf{score}(query\_words[i], source\_words[j]) \\
      H_{i-1,j} + \mathbf{gap\_penalty} \\
      H_{i, j-1} + \mathbf{gap\_penalty}.
    \end{cases}
  $$
  We then take matches by finding entries of the matrix above a certain score threshold $\beta$. Note that we do this on a word by word basis, as discussed above. Also note that the runtime is at least $O(mn)$, which is &lt;em&gt;slow&lt;/em&gt;. I admit that when I initially saw this runtime, as a theoretician, I was like oh good it's polynomial! What a fast algorithm, this is perfect. Then I ran it on my laptop and realized why I usually stick to theory.
&lt;/p&gt;

&lt;p&gt;
  Next, I claim that this algorithm, like the edit distance algorithm, is optimal! But what does that even mean here? And what are reasonable &lt;b&gt;score&lt;/b&gt; and &lt;b&gt;gap_penalty&lt;/b&gt; functions?? Great questions. The answer to the first question is rather dumb: the algorithm is optimal for whatever score/gap penalty functions you choose, so you could choose some nonsense functions and your algorithm will output nonsense. At least it will be optimal nonsense, I suppose. So let's talk about reasonable scoring and gap penalty functions.
&lt;/p&gt;

&lt;h4 class=&quot;subsection-heading&quot;&gt;Smith-Waterman Scoring Functions&lt;/h4&gt;

&lt;p&gt;
  An example of a really simple (and canonical) score function could be as follows: $\mathbf{score}(w_1, w_2) = 1$ if $w_1 = w_2$, otherwise $\mathbf{score}(w_1, w_2) = -p$. Usually $p$ will be in the range of roughly $(0.05, 0.5)$. Taking $p$ too large doesn't allow for hardly any mismatches (aka &quot;fuzziness&quot;), while $p$ being too small will result in very fuzzy matches, to the point where a human would usually conclude that this is not a match. As for the gap penalty function, I think a good rule of thumb is to penalize gaps and misamtches the same, so $\mathbf{gap\_penalty} = -p$.
&lt;/p&gt;

&lt;p&gt;
  Now, I wasn't exactly satisfied with this score function, and so I had what I thought was a really cool idea. My idea was to use a (pretrained) word embedding as a score function to capture semantic similarities! $ \mathbf{score}(w_1, w_2) = $
  $$ 
  \begin{cases}
      -p &amp; \text{ if } \mathsf{cs}(E(w_1), E(w_2)) &lt; \alpha \quad  \text{(mismatch)}\\
      2\cdot (\mathsf{cs}(E(w_1), E(w_2)) -\alpha) &amp; \text{ if } \mathsf{cs}(E(w_1), E(w_2)) \geq \alpha. \quad \text{(match)}
  \end{cases}
  $$
  where $\mathsf{cs}$ is the &lt;em&gt;cosine similarity&lt;/em&gt;, and $E$ is the pretrained word embedding. For concreteness, just consider $\alpha = 0.5$. What this score function does is it sees for example $w_1 = amazing$ and $w_2 = incredible$, then it may not count it as a mismatch, but rather assign a positive score! Otherwise, if we are below the threshold, then it just applies the same constant penalty. I thought this was cool and would've been surprised if no one had done it before. Turns out, someone &lt;em&gt;has&lt;/em&gt; done it before, but I couldn't find anything from before 2020. Here is what some postdoc did last year: &lt;a href=&quot;https://static1.squarespace.com/static/59e25521b7411c07ef1410fa/t/5d499af08dc10f0001ab7e1d/1565104881203/Furnas_SemanticSmithWaterman.pdf&quot;&gt;link&lt;/a&gt;. It's basically the same thing, but he uses a slightly different score function! Anyway, I was pleased with how this score function seemed to work in practice.
&lt;/p&gt;

&lt;h3 class=&quot;subsection-heading&quot;&gt;Combining Greediness and Optimality!&lt;/h3&gt;

&lt;figure&gt;
  &lt;img src=&quot;../../../../img/posts/plagiarism/pen-apple.gif&quot; alt=&quot;Pen pineapple apple pen.&quot; style=&quot;width:100%&quot;&gt;
  &lt;figcaption&gt;A better explanation of what we are about to do than I could ever give.&lt;/figcaption&gt;
&lt;/figure&gt;

If you don't get this reference then you are either much older than me or much younger than me (or we had different childhoods) and either way it's really not that important. 
&lt;/p&gt;

&lt;p&gt;

  You're still here? Incredible. We are now going to do the (perhaps obvious) thing and simply combine Greedy Tiling and Smith-Waterman! In more detail, for each of the $k$ documents retrieved during source retrieval we do the following:

  &lt;ol&gt;
  
    &lt;li&gt;
      Find all the exact matches of length at least $s$ words (I usually pick $s$ to be quite low, say 2 or 3) between the source document and the query document with Greedy Tiling.
    &lt;/li&gt;
    &lt;li&gt;
      For each exact match found in the previous step, look at the $\ell$ words (I think $\ell = 50$ is a reasonable number to start off with, but it depends on the application) preceding and coming after the match in both the query and the source document, and run the Smith-Waterman on these $s + 2\ell$ words, considering only the top match in this range. If the score of the match according to Smith-Waterman is higher than some threshold $\beta$, then keep the match, otherwise discard it.
    &lt;/li&gt;
  &lt;/ol&gt;

  And that's it! If you want to get really fancy you could make $\ell$ not fixed but rather dynamic, increasing as needed. Below is a nice example of how the overall algorithm works:

  &lt;figure&gt;
    &lt;img src=&quot;../../../../img/posts/plagiarism/combination.png&quot; alt=&quot;A cherrypicked example&quot; style=&quot;width:100%&quot;&gt;
    &lt;figcaption&gt;In the above, Greedy Tiling finds the exact match &quot;one of the 52 &quot;Best Loved Books of the Twentieth Century&quot;.&quot;, and then expands it using Smith-Waterman to catch a larger portion that has likely been plagiarized, with some obfuscation!&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;
  Note also that, by design, the Greedy Tiling phase catches proper nouns/names with at least 2-3 words, but if the surrounding context is unrelated in the source and query document then Smith-Waterman will recognize this and discard the match. I think that's pretty nice! Finally, I'll note that this combination of Greedy Tiling and Smith-Waterman was at least partially inspired by the (apparently famous) &lt;a href=&quot;https://en.wikipedia.org/wiki/BLAST_(biotechnology)&quot;&gt;BLAST algorithm&lt;/a&gt;&quot; from bioinformatics, which does something quite similar. 

&lt;/p&gt;


&lt;p&gt;
  Finally, I'll note that this is just one way I thought was nice, efficient, and seemed to work well. There are doubtless many other text alignment texhniques you could go for, and a rather simple solution if you have a good exact match finder is to replace each word with a set of synonyms (and count two words as matching if they are in the same synonym set). One could also try and use some neural network approach that &quot;understands&quot; semantic similarities between two passages, but I have the feeling this would have a significantly slower runtime.
&lt;/p&gt;

&lt;p&gt;
  Now back to theory!
&lt;/p&gt;</content><author><name>Michael Whitmeyer</name></author><category term="plagiarism" /><summary type="html">figure { border: 1px #000000 solid; padding: 4px; margin: auto; }</summary></entry><entry><title type="html">Strong Parallel Repetition … is False</title><link href="https://mwhitmeyer.github.io/blog/parallel-repetition/2021/01/12/parrep.html" rel="alternate" type="text/html" title="Strong Parallel Repetition … is False" /><published>2021-01-12T19:45:13-08:00</published><updated>2021-01-12T19:45:13-08:00</updated><id>https://mwhitmeyer.github.io/blog/parallel-repetition/2021/01/12/parrep</id><content type="html" xml:base="https://mwhitmeyer.github.io/blog/parallel-repetition/2021/01/12/parrep.html">&lt;!-- &lt;h2 class=&quot;section-heading&quot;&gt;Background&lt;/h2&gt; --&gt;
&lt;p&gt;
Before I begin, &lt;a href=&quot;https://homes.cs.washington.edu/~jrl/&quot;&gt;James Lee&lt;/a&gt; typed up some wonderful &lt;a href=&quot;https://tcsmath.wordpress.com/2008/09/21/lecture-1-cheating-with-foams/&quot;&gt;notes&lt;/a&gt; on parallel repetition, in particular strong parallel repetition, and its connection to the unique games conjecture via max-cut. The latter parts of these notes (and in particular lecture 2) dive into the connections between the odd-cycle game and foams. What are foams? Foams are just tilings of \( \mathbb{R}^d \) by \( \mathbb{Z}^d \) by tiles of unit volume. For an (in fact, optimal) example in \( \mathbb{R}^2 \), see the background image at the top of this post.

&lt;/p&gt;

&lt;p&gt;

  I hope to understand this connection with foams a bit better in the future (it's really quite beautiful and clever), but James Lee's notes are already quite nice on this subject, so in this post I am going to avoid beating the dead horse and instead talk about Ran Raz's counterexample to strong parallel repetition. But first, what is strong parallel repetition? What is parallel repetition? There's a lot of background to discuss here, and I will discuss it briefly, but if anything is unclear I refer you to the rest of the internet for sufficient background in order to not make this post ridiculously long.
&lt;/p&gt;
&lt;h2 class=&quot;section-heading&quot;&gt;Some Background&lt;/h2&gt;
&lt;p&gt;
  So first, we are going to concern ourselves with 2 prover 1 round games. In this setting, two provers (or players) \(P_1\) and \(P_2\), lets call them Alice and Bob, are trying to convince a verifier \(V\) of something. The verifier samples some \(x \in X\) and gives it to Alice, and also samples some \(y \in Y\) and gives it to Bob. Alice and Bob then, without communicating, must send back \(a\in A\) and \(b \in B\), respectively. \(X\) and \(Y\) are the input alphabets (over which there is some distribution \(\mu\) that the verifier and the provers both know) for Alice and Bob, and \(A\) and \(B\) are the outputs of Alice and Bob. The last thing we need to define the game is the verifier's &lt;em&gt;predicate&lt;/em&gt;, which we abuse notation and denote as \(V(x,y,a,b)\). It outputs 1 if the verifier accepts Alice and Bob's answers, and 0 otherwise. Then, the value of the game \(\mathcal{G}\) is 

  $$\mathsf{val}(\mathcal{G}) = \max_{f_A, f_B}\Pr[V(x,y,f_A(x), f_B(y)) = 1],$$
  where \(f_A\) and \(f_B\) are Alice and Bob's &quot;strategies&quot;. Note that in order for the game to be interesting at all, whatever Aice and Bob are trying to convince the verifier of should not (always) be true. Otherwise, the value of the game is 1, and there is not much to analyze here. I will also note that Alice and Bob &lt;em&gt;are&lt;/em&gt; allowed to have shared randomness, but this does not help them. This is because any randomized strategy is just a distribution over deterministic strategies, so they can just pick the best of the deterministic strategies within their distribution.

&lt;/p&gt;

&lt;p&gt;
  Now that we have established what a (2 player 1 round) game is, we are ready to introduce the notion of parallel repetition. Suppose we have some game \(\mathcal{G}\) with \(\mathsf{val}(\mathcal{G}) = 1-\epsilon\). For any repetition of \(\mathcal{G}\), we say Alice and Bob &quot;win&quot; iff they win every single repetition. Clearly, if we repeat this game &lt;em&gt;in series&lt;/em&gt; \(n\) times then the value of this repeated game will decay exponentially. The central question that we are concerned with here, however, is what happens when we repeat this game &lt;em&gt;in parallel&lt;/em&gt;? Repeating the game in parallel means that instead of the verifier sending only one \(x\in X\) and one \(y \in Y\) to Alice and Bob respectively, instead the verifier sends \(x_1,...,x_n \in X\) and \(y_1, ... ,y_n \in Y\) and gets back from Alice and Bob \(a_1,...,a_n\) and \(b_1,...,b_n\), respectively. Let's denote this parallel repeated game as \(\mathcal{G^n}\). Then the value of this parallel repeated game is just the probability that Alice and Bob satisify every predicate, i.e. \(V(x_i, y_i, a_i, b_i) = 1 \,\, \forall i = 1,...,n\). 
&lt;/p&gt;
&lt;p&gt;
  What is the value of such a parallel repeated game? One might hope or even assume (as some did back when this problem first began gaining attention), that \(\mathsf{val}(\mathcal{G}^n) = (1-\epsilon)^n\); i.e. that the value of the repeated game decays exactly exponentially. 
&lt;/p&gt;
&lt;p&gt;
  This turns out to be false, which was first shown by Fortnow, and later an even simpler example was described by Feige. The example is instructive, and goes as follows. Consider a game $\widetilde{\mathcal{G}}$ where Alice gets $x \in \{0,1\}$ and Bob gets $y \in \{0,1\}$, and the output alphabets are $A = B = \{1, 2\} \times \{0,1\}$. The verifier's predicate is that both Alice and Bob return either $(1,x)$ or $(2,y)$. It shouldn't be too hard to convince yourself that the value of this game is $1/2$; the best Alice and Bob can do is decide on whether to reply with $1$ or $2$, and then at least one of them has to guess (and with probability $1/2$ they will be right). What happens if we repeat this game in parallel? Here, Alice receives $(x_0, x_1)$ and Bob receives $(y_0, y_1)$. Here is a clever strategy: Alice can return to the verifier $[(1,x_0), (2, x_0)]$ and Bob can return $[(1,y_1), (2, y_1)]$. If $x_0 = y_1$, then Alice and Bob win the parallel repeated game, and this happens with probability $1/2$! Clearly, since the value of the single game is $1/2$, this is also a lower bound for the value of the repeated game, so it turns out the $\mathsf{val}(\widetilde{\mathcal{G}}^2) = 1/2$ for Feige's game.
&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;(Strong) Parallel Repetition&lt;/h2&gt;
&lt;p&gt;
  So what do we know about parallel repetition? Clearly, the above example tells us we cannot have &quot;pure&quot; exponential decay in the value of the game if we repeat in parallel. Does the value even have to go to zero as the number of parallel repetitions goes to infinity? Can we have close to exponential decay? Thankfully, for two player one round games, the answer to both of the questions is essentially less (for more than two players, we know that the value goes to zero very slowly, but we don't know very much beyond that). In particular, in the 1990's Verbitsky showed that the answer to the first question is yes using a deep result in Ramsey theory (although, conditioned on knowing that result, the proof is actually quite cute). However, the decay was &lt;em&gt;super&lt;/em&gt; slow (shown to be on the order of the inverse Ackermann function, Google it, it's crazy). Shortly thereafter, Ran Raz proved his famous parallel repetition theorem, which showed that for some 2P1R game $\mathcal{G}$, if $\mathsf{val}(\mathcal{G}) = 1-\epsilon$, then we have that 
  $$\mathsf{val}(\mathcal{G}^n) \leq (1-O(\epsilon^c))^{\Omega(n/\log(|A||B|))}. $$
  In Raz's original proof (which is, some might argue, a bit less &quot;cute&quot; than Verbitsky's, albeit with the benefit of actually achieving exponential decay), $c=32$, but Holenstein in ~2006 was able to significantly simplify and improve Raz's result to achieve $c=3$ for general 2P1R games. Not long after, for the specific case of &lt;em&gt;projection&lt;/em&gt; 2P1R games, Anup Rao showed that if $\mathsf{val}(\mathcal{G}) = 1-\epsilon$, then we have that 
  $$\mathsf{val}(\mathcal{G}^n) \leq (1-O(\epsilon^2))^{\Omega(n)}. $$
  Note in particular there is no longer dependence on Alice and Bob's output sizes, and the improved exponent. Strong parallel repetition asks what is the smallest exponent we can achieve, for general 2P1R games or even the specific case of projection games? Rao showed than $c\leq 2$. Is it possible that $c &lt; 2$?
&lt;/p&gt;
&lt;p&gt;
  If I had the energy and/or the expertise, now would be a good time to go on a diatribe about why we actually care about optimizing this exponent so much, or indeed why we care about parallel repetition at all. It's an interesting mathematical question! Beyond that, its actually very important in the context of using PCP's to prove hardness of approximation results. At an even finer level of detail, in James Lee's notes linked above he describes how if indeed $c&lt; 2$, even for projection games, this could possibly lead towards a proof of the unique games conjecture. Specifically, if one could show a strong hardness of approximation result for Max-Cut (without assuming UGC), and $c&lt; 2$ (i.e. we had some form of &quot;strong&quot; parallel repetition), then this would imply the unique games conjecture! Again, I will avoid beating the dead horse here since this is already discussed more nicely than I probably would be able to in James Lee's blog. Just trying to give some semblance of motivation here. 
&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Raz's Counterexample&lt;/h2&gt;
&lt;p&gt;
Ok, now that I've gotten your hopes up about just a couple of the cool reasons people would like to see a strong parallel repetition theorem, now I am ready to disappoint. Well, I am always ready to disappoint, but this time I've specifically prepared for the disappointing I am about to do. Let's get into it! 
&lt;/p&gt;
&lt;p&gt;
  First, I need to introduce the notion of an &lt;em&gt;odd-cycle game&lt;/em&gt;. The idea is very simple: Alice and Bob have a cycle on $m=2k+1$ vertices, let's call this graph $C_m$, and are trying to convince the verifier that this graph is 2-colorable. Clearly, it is not (and if it were, then this would not be a very interesting game because Alice and Bob would always be able to convince the verifier). To see this, consider the below picture, where it is clear that at least two neighboring vertices must be colored different colors (or Alice and Bob must have different answers about the coloring).
&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&quot;../../../../img/posts/parrep/oddcycle.png&quot; alt=&quot;Odd-Cycle Game&quot; style=&quot;width:250px;height:275px;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;
  This is $C_5$, where I have labeled each vertex in $[-k,k]$, following the convention of Raz. I have also identified each edge (of which there are $m$), with its opposite vertex (also Raz's convention). The game works as follows: with probability $1/2$, the verifier will ask Alice and Bob about (the same) random vertex, and expect Alice and Bob to answer with the same color, and with probability $1/2$, the verifier will ask Alice and Bob about two neighboring vertices (so an edge), and will accept their answers if they give different labels for these two vertices. I've given a sample strategy for Alice and Bob above by choosing a coloring of the vertices, and it is not hard to see that when Alice and Bob do this, the value of the game is $1-\frac{1}{2m}$. It turns out that this is the best possible strategy. 
&lt;/p&gt;
&lt;p&gt;
  This game turns about to be very interesting due to its relationship with foams mentioned in the first paragraph, and when this was first being explored it was shown by Feige, Kindler, and O'Donnell that $\mathsf{val}(G_{C_m}^n) \leq 1 - \frac{1}{m}\widetilde{\Omega}(\sqrt n)$, where $G_{C_m}$ is this odd-cycle game. We won't get much into this though (maybe in a later post...). What we will show today is that this is actually tight, and there is a protocol that Alice and Bob can run which achieves (for some $n \leq O(m^2)$):
  $$\mathsf{val}(G_{C_m}^n) \geq 1 - \frac{1}{m}O(\sqrt n).$$
  Recall that $n$ is the number of parallel repetitions and $m=2k+1$ is the number of vertices in our odd-cycle. This is actually precisely the counterexample to strong parallel repetition that Raz proved! Why is it a counterexample? Well, consider $n \geq \Omega(m^2)$. Then, we can split $[n]$ into roughly $\frac{n}{\alpha m^2}$ blocks, each of size $\alpha m^2$. Then, Alice and Bob can run this protocol on each block, and we would have that 
  $$\mathsf{val}(G_{C_m}^n) = \left(1 - \frac{1}{m}O(\sqrt{\text{block size}})\right)^{\text{# of blocks}}  \approx (1-\frac{1}{4m^2})^{O(n)}.$$
  The $\approx$ above just hides some computations/constants. The important thing is that this gives a protocol that achieves a value of $\Omega(1-\epsilon^2)^{O(n)}$ for the odd-cycle game, which serves as a counterexample to strong parallel repetition.
&lt;/p&gt;
&lt;h1 class=&quot;section-heading&quot;&gt;Proof Idea&lt;/h1&gt;

&lt;p&gt;
  I will work backwards in sketching this proof idea. Namely, let's start with some wishful thinking. Recall that Alice receives $x_1,...,x_n \in [-k,k]$ as her input and Bob receives $y_1,...,y_k \in [-k,k]$ as his input such that $y_i=x_i$ with probability $1/2$, and $y_i = \pm x_i$ with probability $1/2$. Suppose Alice and Bob could (potentially using shared randomness) agree on a set of edges $e_1,...,e_n$ of edges such that $e_i$ does not have $x_i$ as one of its endpoints for all $i \in [n]$. Then, I claim that Alice and Bob can win the repeated game with probability one. Why? Well, once they have agreed on these $n$ edges, on repetition $i$ they can simply color both vertices blue that touch $e_i$, and alternate all the other colors. We know that $x_i$ does not touch $e_i$, so the verifier will never check this edge, and Alice and Bob are guaranteed to win. 
&lt;/p&gt;
&lt;p&gt;
  So, I have argued that all Alice and Bob need to do is agree on $e_1,...,e_n$ with this property with probability at least $1-\frac{1}{m}O(\sqrt n)$. Now, the goal is to show how to use shared randomness to actually achieve this. To do so, we need a lemma from Holenstein, which was actually used to simplify Raz's original proof of his parallel repetition theorem. Here is the lemma:
  &lt;div class=&quot;lemma&quot;&gt;
    Let $W$ be some finite sample space. Suppose Alice can construct $P_A: W \to \mathbb{R}$ and Bob has $P_B: W \to \mathbb{R}$ such that $\|P_A - P_B\|_1 \leq \delta$. Then, there is a way for Alice to sample $w_A \sim P_A$ and Bob to sample $w_B \sim P_B$ (using their shared random string) such that $w_A = w_B$ with probability $1-O(\delta)$.
    &lt;/div&gt;
    I shan't prove this rigorously, but I think the intuition is quite clear, and goes as follows. Alice and Bob can treat their shared random string $r$ as a sequence of tuples 
    $$r = (r_1,w_1),(r_2,w_2),....$$
    where $r_i \sim Unif[0,1]$ can be thought of as a uniformly random (real) number between zero and one, and each $w_i \sim W$ uniformly. Alice will then choose the first $w_i$ such that $r_i \leq P_A(w_i)$ and Bob will choose the first $w_j$ such that $r_j \leq P_B(w_j)$. If $P_A$ is statistically close to $P_B$, then the idea is that $i=j$ with high probability, and this can be quantified (there are also, I imagine, some technical details to deal with regarding the fact that we are using bits to sample a real number). Also, it is clear that marginally, Alice is sampling according to $P_A$ and Bob according to $P_B$. Pretty clever!
&lt;/p&gt;

&lt;p&gt;

  With Holenstein's lemma in hand, we are much closer. Alice and Bob's strategy will be as follows. Alice will define $P_x(e)$, which is a probability distribution on edges, defined by her input $x = (x_1,...,x_n)$, which gives the probability of her sampling a particular $e = (e_1,...,e_n)$. Bob will do the same and define $P_y(e)$. It suffices for $P_x$ and $P_y$ to have the property that 1) the statistical distance of $P_x$ and $P_y$ is on the order of $\frac{1}{m}O(\sqrt n)$ and 2) the probability that $x_i$ touches $e_i$ for any of the $e_i$ that they sample is negligible (or at least less than $\frac{1}{m}O(\sqrt n)$). By Holenstein's lemma, we would then be done! This is in fact the only technical part of Raz's proof. The distributions $P_x$ and $P_y$ are defined as product distributions over the individual rounds. Basically, suppose Alice gets $x_i$ and Bob gets $y_i \in \{x_i - 1, x_i, x_i +  1\} (\text{mod $m$})$. We just need to define a distribution that places small weight on edges that are touching/close to $x_i$ (in order to satisfy the condition that $e_i$ does not touch $x_i$), while also being &lt;em&gt;smooth&lt;/em&gt; enough such that the statistical distance is not too much between $P_x$ and $P_y$. He achieves this by using a normalized quadratic function that has its maximum at the edge that is furthest away from the current vertex. 
&lt;/p&gt;
&lt;p&gt;
  This is pretty much where my intuition stops (if you want to see how Raz defined $P_x$ and $P_y$, you can read through the ~1.5 pages of math in his paper that would not be so instructive for me to copy here). However, Justin Holmgren pointed me to perhaps some more intuition via this &lt;a href=&quot;https://eprint.iacr.org/2017/537&quot;&gt;paper&lt;/a&gt;. Essentially, this paper explores what they call the &quot;chi-squared&quot; method, which is a way to bound the statistical distance of two product distributions using the chi-squared distance of the distributions of which they are made. Suppose (and this is indeed the case), that $P_x$ is a $n$-wise direct product of distributions. The &quot;naive&quot; way to bound the statistical distance of $P_x$ and $P_y$ would be to just multiply by $n$ the statistical distance of the individual distributions of which it is comprised. However, this paper I linked above shows that if you can bound the chi-squared distance of the individual distributions, then the statistical distance is bounded by something like $\sqrt n \delta$ (where $\delta^2$ is the chi-squared distance of the individual distributions). I think that the third condition Raz lists when defining the individual distributions of which $P_x$ and $P_y$ are comprised somehow correspond to a chi-squared distance bound. This is still a bit fuzzy to me though, and I haven't exactly formalized how this third condition corresponds to a chi-squared bound yet.
&lt;/p&gt;

&lt;p&gt;
  Whew. That's all for this post!
&lt;/p&gt;</content><author><name>Michael Whitmeyer</name></author><category term="parallel-repetition" /><summary type="html">Before I begin, James Lee typed up some wonderful notes on parallel repetition, in particular strong parallel repetition, and its connection to the unique games conjecture via max-cut. The latter parts of these notes (and in particular lecture 2) dive into the connections between the odd-cycle game and foams. What are foams? Foams are just tilings of \( \mathbb{R}^d \) by \( \mathbb{Z}^d \) by tiles of unit volume. For an (in fact, optimal) example in \( \mathbb{R}^2 \), see the background image at the top of this post.</summary></entry><entry><title type="html">Intro</title><link href="https://mwhitmeyer.github.io/blog/intro/2021/01/06/intro.html" rel="alternate" type="text/html" title="Intro" /><published>2021-01-06T00:00:00-08:00</published><updated>2021-01-06T00:00:00-08:00</updated><id>https://mwhitmeyer.github.io/blog/intro/2021/01/06/intro</id><content type="html" xml:base="https://mwhitmeyer.github.io/blog/intro/2021/01/06/intro.html">&lt;h2 class=&quot;section-heading&quot;&gt;Hi There&lt;/h2&gt;
&lt;p&gt;
On this blog you will find a disorganized mix of theoretical computer science and general musings about literally anything else I find interesting. Hence the name of the blog (which is certainly not as clever as I think it is). A nice diverse potpourri/amalgam/melange of topics is healthy right? Says the guy who finds something he likes and ignores literally everything else. Anyway, this blog is for me, as I venture forth into the vast (and intimidating) depths of both theoretical computer science and life. With any luck, this blog will keep me entertained, learning, and accountable. 
&lt;/p&gt;

&lt;p&gt;
  I also, clearly, am a fan of Calvin and Hobbes. If you are not familiar, I am not referring to the philosopher and the theologian, but rather the (far more enlightening) comic strip series by Bill Watterson, which premiered in newspapers between 1985 and 1995 and featured the precocious youth Calvin and his anthropomorphic tiger Hobbes. I am also fond of run-on sentences, so deal with it (or leave), this is my blog. Every post (for the forseeable future) will feature a quote from Calvin and Hobbes that, at least in some convoluted way, has to do with that blog post. For example, I am winging both this blog and my life this year, which is also coincidentally my new year's resolution. So this quote's connection isn't even convoluted!
&lt;/p&gt;

&lt;p&gt;

  Anyway, that's all for now. More substantive posts to come.
&lt;/p&gt;</content><author><name>Michael Whitmeyer</name></author><category term="intro" /><summary type="html">Hi There On this blog you will find a disorganized mix of theoretical computer science and general musings about literally anything else I find interesting. Hence the name of the blog (which is certainly not as clever as I think it is). A nice diverse potpourri/amalgam/melange of topics is healthy right? Says the guy who finds something he likes and ignores literally everything else. Anyway, this blog is for me, as I venture forth into the vast (and intimidating) depths of both theoretical computer science and life. With any luck, this blog will keep me entertained, learning, and accountable.</summary></entry></feed>