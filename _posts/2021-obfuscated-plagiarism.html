---
layout: post
title: "Efficiently Detecting Obfuscated Plagiarism"
subtitle: "\"People pay more attention when they think youâ€™re up to something.\"  <br /> -Bill Watterson "
date: 2021-08-21 23:45:13 
background: '/img/posts/plagiarism/blast.jpeg'
categories: plagiarism
published: true
---

<p>

  Hi! It's been awhile. Quarantine hasn't been all fun and daisies for me personally (or most other people I imagine), but I am trying to get myself motivated and back into the swing of things. But I am not here to bore you with all that. Suffice it to say that hopefully I will start posting more regularly now as I head into my PhD!

</p>

<p>

  Now, this post will be about what I learned and created this summer during my internship at <a href="https://www.stottlerhenke.com/">Stottler Henke</a> (SHAI). In general, I don't plan on discussing such (gasp) <em>real-world applicable</em> things going forward on this blog, in case anyone was concerned.

</p>

<h2 class="section-heading">Problem Setup</h2>


<p>
  Now, let's talk about plagiarism. Here's the setup: we have a (very) large corpus (say, Wikipedia) of $N$ "sensitive" or "source" documents, which may or may not have been plagiarized by our query document. When we say plagiarism, we don't necessarily mean the whole document -- it could just be that a key sentence was copied, or a paragraph with crucial information was summarized at some (potentially random) point within the query document. Now, for exact matches, this is a relatively straightforward task, although it takes some clever data structures that I won't discuss in this post to get the runtime to depend only (linearly) on the length $m$ of the query document rather than $N$ (all the source documents we have to search through). This is what had already been accomplished when I arrived for my internship, and my task was to see what might be possible when our job was not just ot find exact matches, but also "fuzzy" matches. But what does "fuzzy" even mean? So glad you asked, I still have no idea.
</p>

<p>
  Nevertheless, onwards. It seemed as good a starting place as any was the <a href="https://pan.webis.de/clef14/pan14-web/">PAN Conference</a>, a conference whose central theme in 2011-2015 was plagiarism detection! Now, this seemed to be more of a competition and less of a thoroughly peer-reviewed venue, but nonetheless there were some interesting ideas floating around there, in case you want to see other things outside of what I will talk about in this post. In addition, they had a nice (albeit small) dataset and evaluation framework (which were a bit difficult to find but not impossible) that allows for an objective scoring and evaluation of any algorithm one might come up with! Beyond the dataset (and floating ideas), and perhaps most importantly, they also had a very nice framework for plagiarism detection, which I will now describe.
</p>

<p>
  The framework they describe splits plagiarism detection into two main tasks: 

  <ol>
    <li><b>Source Retrieval:</b> This refers to the task of (efficiently) narrowing our search down from $N$ (which is just <em>overwhelmingly</em> large) possible source documents to just $k$ (which we think of as some reasonable constant). We will do a more detailed plagiarism detection analysis on these $k$ documents in the next phase. You can think about this phase a sort of google search with the query document as the input and the source documents as the possible "websites". Of course, like anyone ever, we only have time to look at the first page (a.k.a. the first $k$) results that Google spits out. </li>
    <li><b>Text Alignment:</b> This refers to the task of, well, actually "finding" the instances of plagiarism between the query document and the top $k$ source documents. This is where the fun really begins!</li>
  </ol>

  <p>
    <img src="../../../../img/posts/plagiarism/src-retrieve-text-align.png" alt="There's supposed to be a visualization of source retrieval and text alignment here." style="width:250px;height:275px;" />
  </p>

  In this post we will discuss some general strategies for source retrieval, but all of them involve some sort of "search engine" such as ElasticSearch or Lucene or maybe Google has an API (I dunno just pick your favorite one I don't find this aspect as exciting).
</p>